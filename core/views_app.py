import threading
import time
import re
import random
from urllib.parse import urljoin, urlparse
from defusedxml.ElementTree import fromstring as ET_fromstring
import requests
from bs4 import BeautifulSoup
from django.contrib.auth.models import User
from django.core.paginator import Paginator
from django.db.models import Q
from django.http import JsonResponse
from django.shortcuts import render
from django.utils import timezone

from .forms import AdminSetPasswordForm, DominioForm
from .models import BusquedaDominio

# Variable global temporal para progreso (en producci√≥n usar cache/db)
crawling_progress = {}

# User-Agents rotativos para evitar detecci√≥n
USER_AGENTS = [
    ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
     '(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 '
     '(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 '
     '(KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'),
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/119.0',
    ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
     '(KHTML, like Gecko) Edge/119.0.0.0'),
    ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 '
     '(KHTML, like Gecko) Version/17.0 Safari/605.1.15')
]


def get_random_headers():
    """Genera headers aleatorios para simular navegador real"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        'Accept': ('text/html,application/xhtml+xml,application/xml;q=0.9,'
                   'image/webp,*/*;q=0.8'),
        "Accept-Language": "en-US,en;q=0.9,es;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0",
    }


def detect_blocking(response, url):
    """Detecta si una respuesta indica bloqueo anti-bot"""
    status = response.status_code
    content = response.text.lower() if hasattr(response, "text") else ""

    # C√≥digos de estado que indican bloqueo
    if status in [403, 429, 503]:
        return True, f"HTTP {status}: Acceso bloqueado por el servidor"

    # Contenido que indica bloqueo
    blocking_keywords = [
        "blocked",
        "forbidden",
        "access denied",
        "cloudflare",
        "captcha",
        "robot",
        "bot detected",
        "rate limit",
        "too many requests",
        "suspicious activity",
    ]

    if any(keyword in content for keyword in blocking_keywords):
        return True, "Contenido indica protecci√≥n anti-bot"

    # Respuesta muy peque√±a o vac√≠a puede indicar bloqueo
    if len(content) < 100 and status == 200:
        return True, "Respuesta sospechosamente peque√±a"

    return False, ""


def try_sitemap_fallback(domain):
    """Intenta obtener URLs del sitemap cuando el crawling falla"""
    sitemap_urls = [
        f"https://{domain}/sitemap.xml",
        f"https://www.{domain}/sitemap.xml",
        f"https://{domain}/sitemap_index.xml",
        f"https://{domain}/sitemaps.xml",
    ]

    # Primero buscar en robots.txt
    try:
        robots_response = requests.get(
            f"https://{domain}/robots.txt", timeout=10, headers=get_random_headers()
        )
        if robots_response.status_code == 200:
            for line in robots_response.text.split("\n"):
                if line.lower().strip().startswith("sitemap:"):
                    sitemap_url = line.split(":", 1)[1].strip()
                    sitemap_urls.insert(0, sitemap_url)
    except:
        pass

    for sitemap_url in sitemap_urls:
        try:
            response = requests.get(
                sitemap_url, timeout=15, headers=get_random_headers()
            )
            if response.status_code == 200:
                return parse_sitemap_urls(response.content, domain)
        except Exception:
            continue

    return []


def parse_sitemap_urls(content, base_domain, max_urls=100):
    """Extrae URLs de un sitemap XML"""
    urls = []
    try:
        root = ET_fromstring(content)

        # Definir namespaces comunes
        namespaces = {
            "sm": "http://www.sitemaps.org/schemas/sitemap/0.9",
            "": "http://www.sitemaps.org/schemas/sitemap/0.9",
        }

        # Buscar URLs con namespace
        url_elements = root.findall(".//sm:url", namespaces)
        if not url_elements:
            # Buscar sin namespace
            url_elements = root.findall(".//url")

        for url_elem in url_elements:
            loc = url_elem.find("sm:loc", namespaces)
            if loc is None:
                loc = url_elem.find("loc")

            if loc is not None and loc.text:
                url = loc.text.strip()
                if url.startswith("http") and base_domain in url:
                    urls.append(url)
                    if len(urls) >= max_urls:
                        break

        # Si no encontramos URLs, buscar sitemaps anidados
        if not urls:
            sitemap_elements = root.findall(".//sm:sitemap", namespaces)
            if not sitemap_elements:
                sitemap_elements = root.findall(".//sitemap")

            for sitemap_elem in sitemap_elements[:5]:  # M√°ximo 5 sitemaps anidados
                loc = sitemap_elem.find("sm:loc", namespaces)
                if loc is None:
                    loc = sitemap_elem.find("loc")

                if loc is not None and loc.text:
                    nested_sitemap_url = loc.text.strip()
                    try:
                        nested_response = requests.get(
                            nested_sitemap_url, timeout=10, headers=get_random_headers()
                        )
                        if nested_response.status_code == 200:
                            nested_urls = parse_sitemap_urls(
                                nested_response.content,
                                base_domain,
                                max_urls - len(urls),
                            )
                            urls.extend(nested_urls)
                            if len(urls) >= max_urls:
                                break
                    except Exception:
                        continue

    except Exception as e:
        print(f"Error parseando sitemap: {e}")

    return urls[:max_urls]


# --- Guardar b√∫squeda desde AJAX ---
def guardar_busqueda_ajax(dominio, urls, user=None):
    # Limpiar: quitar vac√≠os, espacios y duplicados
    urls_limpias = list(dict.fromkeys([u.strip() for u in urls if u and u.strip()]))
    from django.utils import timezone

    # Buscar la √∫ltima b√∫squeda sin fecha_fin para este usuario y dominio

    obj = None
    if user and hasattr(user, "is_authenticated") and user.is_authenticated:
        obj = (
            BusquedaDominio.objects.filter(
                dominio=dominio, usuario=user, fecha_fin__isnull=True
            )
            .order_by("-fecha")
            .first()
        )
    else:
        obj = (
            BusquedaDominio.objects.filter(
                dominio=dominio, usuario=None, fecha_fin__isnull=True
            )
            .order_by("-fecha")
            .first()
        )
    if obj:
        obj.urls = "\n".join(urls_limpias)
        obj.fecha_fin = timezone.now()
        obj.save()
    else:
        BusquedaDominio.objects.create(
            dominio=dominio,
            usuario=(
                user
                if user and hasattr(user, "is_authenticated") and user.is_authenticated
                else None
            ),
            urls="\n".join(urls_limpias),
            fecha=timezone.now(),
        )


def crawl_urls_progress(base_url, max_urls, progress_key):
    visited = set()
    to_visit = [base_url]
    urls = []

    def normalize_netloc(netloc):
        return netloc.lower().replace("www.", "")

    domain = normalize_netloc(urlparse(base_url).netloc or base_url)

    while to_visit:
        url = to_visit.pop(0)
        if url in visited:
            continue
        visited.add(url)
        try:
            resp = requests.get(
                url,
                timeout=8,
                headers={
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/119.0.0.0 Safari/537.36"
                    )
                },
            )
            print(f"[CRAWL] URL: {url} | Status: {resp.status_code}")
            if resp.status_code != 200:
                continue
            soup = BeautifulSoup(resp.content, "html.parser")
            urls.append(url)
            enlaces = [a["href"].strip() for a in soup.find_all("a", href=True)]
            print(f"[CRAWL] Enlaces encontrados en {url}: {len(enlaces)}")
            if enlaces:
                print(f"[CRAWL] Primeros 5 enlaces: {enlaces[:5]}")
            # Actualizar progreso
            crawling_progress[progress_key] = {
                "count": len(urls),
                "last": url,
                "done": False,
            }
            if max_urls and len(urls) >= max_urls:
                break
            for href in enlaces:
                if (
                    href.startswith("#")
                    or href.startswith("mailto:")
                    or href.startswith("javascript:")
                ):
                    continue
                abs_url = urljoin(url, href)
                parsed = urlparse(abs_url)
                # Permitir tanto con como sin www
                if parsed.netloc and normalize_netloc(parsed.netloc) != domain:
                    continue
                if (
                    abs_url not in visited
                    and abs_url not in to_visit
                    and abs_url.startswith("http")
                ):
                    to_visit.append(abs_url)
        except Exception as e:
            print(f"[CRAWL][ERROR] {url}: {e}")
            continue  # nosec
    crawling_progress[progress_key] = {
        "count": len(urls),
        "last": None,
        "done": True,
        "urls": urls,
    }
    return urls


def iniciar_crawling_ajax(request):
    """Inicia el crawling en background y retorna una key de progreso"""
    if request.method == "POST":
        dominio = request.POST.get("dominio")
        limite_urls = request.POST.get("limite_urls")
        try:
            limite_urls = int(limite_urls) if limite_urls else None
        except Exception:
            limite_urls = None

        # Probar primero con https, si falla probar con http
        def limpiar_dominio(d):
            d = d.strip()
            if d.startswith("http://"):
                d = d[7:]
            elif d.startswith("https://"):
                d = d[8:]
            return d.rstrip("/")

        dominio_limpio = limpiar_dominio(dominio)

        def get_working_base_url(dominio):
            for proto in ["https", "http"]:
                url = f"{proto}://{dominio}"
                try:
                    resp = requests.get(
                        url, timeout=6, headers={"User-Agent": "PrestaLab"}
                    )
                    if resp.status_code == 200:
                        return url
                except Exception:
                    continue  # nosec
            return f"https://{dominio}"  # fallback

        base_url = get_working_base_url(dominio_limpio)
        progress_key = f"{dominio}_{int(time.time())}"
        crawling_progress[progress_key] = {"count": 0, "last": None, "done": False}

        # Crear el objeto BusquedaDominio al iniciar
        from django.utils import timezone

        obj = BusquedaDominio.objects.create(
            dominio=dominio,
            usuario=(request.user if request.user.is_authenticated else None),
            urls="",
            fecha=timezone.now(),
        )
        # Guardar el id en la sesi√≥n para referencia
        request.session["busqueda_id"] = obj.id

        def crawl_and_save():
            urls = crawl_urls_progress(base_url, limite_urls, progress_key)
            # Al finalizar, actualizar el objeto con urls y fecha_fin
            obj.urls = "\n".join(urls)
            obj.fecha_fin = timezone.now()
            obj.save()

        t = threading.Thread(target=crawl_and_save)
        t.start()
        return JsonResponse({"progress_key": progress_key})
    return JsonResponse({"error": "M√©todo no permitido"}, status=405)


def progreso_crawling_ajax(request):
    """Devuelve el progreso actual del crawling"""
    key = request.GET.get("progress_key")
    if not key or key not in crawling_progress:
        return JsonResponse({"error": "Clave inv√°lida"}, status=404)
    prog = crawling_progress[key]
    return JsonResponse(prog)


def admin_set_password_view(request, user_id):
    """Vista para que un admin cambie la contrase√±a de cualquier usuario"""
    try:
        usuario = User.objects.get(pk=user_id)
    except User.DoesNotExist:
        return render(
            request,
            "usuarios/cambiar_password.html",
            {"error": "Usuario no encontrado."},
        )

    mensaje = ""
    if request.method == "POST":
        form = AdminSetPasswordForm(request.POST)
        if form.is_valid():
            new_password = form.cleaned_data["new_password1"]
            usuario.set_password(new_password)
            usuario.save()
            mensaje = f"Contrase√±a actualizada para {usuario.username}."
            form = AdminSetPasswordForm()
        else:
            mensaje = "Corrija los errores indicados."
    else:
        form = AdminSetPasswordForm()

    return render(
        request,
        "usuarios/cambiar_password.html",
        {"form": form, "usuario": usuario, "mensaje": mensaje},
    )


def listar_usuarios_view(request):
    """
    Vista para listar todos los usuarios del sistema.
    Permite filtrar por nombre, email y tipo (admin/lectura).
    """
    q = request.GET.get("q", "").strip()
    tipo = request.GET.get("tipo", "")
    usuarios = User.objects.all()
    if q:
        usuarios = usuarios.filter(username__icontains=q) | usuarios.filter(
            email__icontains=q
        )
    if tipo == "admin":
        usuarios = usuarios.filter(is_staff=True)
    elif tipo == "lectura":
        usuarios = usuarios.filter(is_staff=False)
    usuarios = usuarios.order_by("-date_joined")
    return render(request, "usuarios/listar_usuarios.html", {"usuarios": usuarios})


def api_status(request):
    """Vista para verificar el estado de la API"""
    return JsonResponse({"status": "ok"})


def normalizar_dominio(dominio_raw):
    """Normaliza un dominio: quita protocolo, path, puerto, www, etc."""
    dominio_raw = dominio_raw.strip().lower()
    dominio = re.sub(r"^https?://", "", dominio_raw)
    dominio = dominio.split("/")[0].split("?")[0]
    dominio = dominio.split(":")[0]
    partes = dominio.split(".")
    if len(partes) >= 3 and partes[0] == "www":
        dominio = ".".join(partes[1:])
    dominio = dominio.rstrip(".")
    dominio = re.sub(r"\.{2,}", ".", dominio)
    return dominio


def crawl_urls(base_url, max_urls=None):
    """Funci√≥n auxiliar mejorada para crawlear URLs de un dominio"""
    visited = set()
    to_visit = [base_url]
    urls = []
    domain = urlparse(base_url).netloc or base_url
    blocked_count = 0
    max_blocks = 3  # M√°ximo de bloqueos antes de cambiar estrategia
    crawl_delay = 1  # Delay inicial en segundos

    print(f"[CRAWL] Iniciando crawling mejorado de {base_url}")
    print(f'[CRAWL] L√≠mite de URLs: {max_urls or "Sin l√≠mite"}')

    # Verificar robots.txt para obtener delay recomendado
    try:
        robots_url = f"https://{domain}/robots.txt"
        robots_response = requests.get(
            robots_url, timeout=10, headers=get_random_headers()
        )
        if robots_response.status_code == 200:
            for line in robots_response.text.split("\n"):
                if line.lower().strip().startswith("crawl-delay:"):
                    try:
                        recommended_delay = int(line.split(":", 1)[1].strip())
                        crawl_delay = max(crawl_delay, recommended_delay)
                        print(
                            f"[CRAWL] Delay recomendado por robots.txt: {crawl_delay}s"
                        )
                    except:
                        pass
                elif line.lower().strip().startswith("disallow: /"):
                    print(f"[CRAWL] ‚ö†Ô∏è robots.txt proh√≠be el crawling completo")
    except:
        pass

    while to_visit and len(urls) < (max_urls or float("inf")):
        url = to_visit.pop(0)
        if url in visited:
            continue
        visited.add(url)

        try:
            # Aplicar delay inteligente
            if len(urls) > 0:  # No delay en la primera request
                delay = crawl_delay * (
                    1 + blocked_count * 0.5
                )  # Aumentar delay si hay bloqueos
                print(
                    f"[CRAWL] Esperando {delay:.1f}s antes de la siguiente request..."
                )
                time.sleep(delay)

            # Request con headers aleatorios
            headers = get_random_headers()
            resp = requests.get(url, timeout=15, headers=headers)

            print(f"[CRAWL] {url} -> {resp.status_code}")

            # Detectar bloqueos
            is_blocked, block_reason = detect_blocking(resp, url)

            if is_blocked:
                blocked_count += 1
                print(f"[CRAWL] ‚ö†Ô∏è BLOQUEO DETECTADO: {block_reason}")

                if blocked_count >= max_blocks:
                    print(
                        f"[CRAWL] üö® Demasiados bloqueos ({blocked_count}). Cambiando a estrategia de sitemap..."
                    )
                    sitemap_urls = try_sitemap_fallback(domain)
                    if sitemap_urls:
                        print(
                            f"[CRAWL] ‚úÖ Sitemap encontrado con {len(sitemap_urls)} URLs"
                        )
                        urls.extend(
                            sitemap_urls[
                                : (
                                    max_urls - len(urls)
                                    if max_urls
                                    else len(sitemap_urls)
                                )
                            ]
                        )
                    return {
                        "urls": urls,
                        "status": "blocked_fallback_sitemap",
                        "message": f"Crawling bloqueado despu√©s de {blocked_count} intentos. Se us√≥ sitemap como alternativa.",
                        "blocked_count": blocked_count,
                        "sitemap_urls": len(sitemap_urls) if sitemap_urls else 0,
                    }

                # Aumentar delay y continuar
                crawl_delay *= 2
                continue

            if resp.status_code != 200:
                print(f"[CRAWL] Status no exitoso: {resp.status_code}")
                continue

            soup = BeautifulSoup(resp.content, "html.parser")
            urls.append(url)
            print(f"[CRAWL] ‚úÖ URL agregada. Total: {len(urls)}")

            if max_urls and len(urls) >= max_urls:
                print(f"[CRAWL] üéØ L√≠mite alcanzado: {max_urls} URLs")
                break

            # Extraer enlaces
            links_found = 0
            for a in soup.find_all("a", href=True):
                href = a["href"].strip()
                if (
                    href.startswith("#")
                    or href.startswith("mailto:")
                    or href.startswith("javascript:")
                    or href.startswith("tel:")
                    or href.startswith("ftp:")
                ):
                    continue

                abs_url = urljoin(url, href)
                parsed = urlparse(abs_url)

                # Normalizar dominio para comparaci√≥n
                def normalize_domain(d):
                    return d.lower().replace("www.", "")

                if parsed.netloc and normalize_domain(
                    parsed.netloc
                ) != normalize_domain(domain):
                    continue

                if (
                    abs_url not in visited
                    and abs_url not in to_visit
                    and abs_url.startswith("http")
                    and len(to_visit) < 1000  # Evitar cola infinita
                ):
                    to_visit.append(abs_url)
                    links_found += 1

            print(f"[CRAWL] Enlaces internos encontrados: {links_found}")

        except requests.exceptions.Timeout:
            print(f"[CRAWL] ‚è∞ Timeout en {url}")
            blocked_count += 1
            if blocked_count >= max_blocks and len(urls) == 0:
                print(f"[CRAWL] üö® Demasiados timeouts. Intentando sitemap...")
                sitemap_urls = try_sitemap_fallback(domain)
                if sitemap_urls:
                    print(f"[CRAWL] ‚úÖ Sitemap encontrado con {len(sitemap_urls)} URLs")
                    urls.extend(sitemap_urls[: max_urls or len(sitemap_urls)])
                return {
                    "urls": urls,
                    "status": (
                        "timeout_fallback_sitemap"
                        if sitemap_urls
                        else "timeout_no_sitemap"
                    ),
                    "message": f'Timeouts repetidos. {"Se us√≥ sitemap como alternativa." if sitemap_urls else "Sin sitemap disponible."}',
                    "blocked_count": blocked_count,
                    "sitemap_urls": len(sitemap_urls) if sitemap_urls else 0,
                }
            continue
        except requests.exceptions.ConnectionError:
            print(f"[CRAWL] üîå Error de conexi√≥n en {url}")
            blocked_count += 1
            if blocked_count >= max_blocks and len(urls) == 0:
                print(
                    f"[CRAWL] üö® Demasiados errores de conexi√≥n. Intentando sitemap..."
                )
                sitemap_urls = try_sitemap_fallback(domain)
                if sitemap_urls:
                    print(f"[CRAWL] ‚úÖ Sitemap encontrado con {len(sitemap_urls)} URLs")
                    urls.extend(sitemap_urls[: max_urls or len(sitemap_urls)])
                return {
                    "urls": urls,
                    "status": (
                        "connection_error_fallback_sitemap"
                        if sitemap_urls
                        else "connection_error_no_sitemap"
                    ),
                    "message": f'Errores de conexi√≥n repetidos. {"Se us√≥ sitemap como alternativa." if sitemap_urls else "Sin sitemap disponible."}',
                    "blocked_count": blocked_count,
                    "sitemap_urls": len(sitemap_urls) if sitemap_urls else 0,
                }
            continue
        except Exception as e:
            print(f"[CRAWL] ‚ùå Error en {url}: {str(e)[:100]}")
            continue

    result = {
        "urls": urls,
        "status": "success",
        "message": f"Crawling completado exitosamente. {len(urls)} URLs encontradas.",
        "blocked_count": blocked_count,
        "total_visited": len(visited),
    }

    print(f"[CRAWL] üèÅ Finalizado: {len(urls)} URLs, {blocked_count} bloqueos")
    return result


def analisis_dominio_view(request):
    """Vista para ingresar dominio y mostrar historial de b√∫squedas"""
    form = DominioForm()
    mensaje = ""

    regex_part1 = r"^[a-z0-9]([a-z0-9-]*[a-z0-9])?"
    regex_part2 = r"(\.[a-z0-9]([a-z0-9-]*[a-z0-9])?)*"
    regex_part3 = r"$"
    regex = regex_part1 + regex_part2 + regex_part3

    if request.method == "POST":
        if "eliminar_individual" in request.POST:
            eliminar_id = request.POST.get("eliminar_individual")
            BusquedaDominio.objects.filter(id=eliminar_id).delete()
            mensaje = "B√∫squeda eliminada correctamente."
        elif "eliminar_seleccionados" in request.POST:
            ids = request.POST.getlist("eliminar_ids")
            BusquedaDominio.objects.filter(id__in=ids).delete()
            mensaje = f"{len(ids)} b√∫squedas eliminadas correctamente."
        else:
            form = DominioForm(request.POST)
            if form.is_valid():
                dominio_raw = form.cleaned_data["dominio"]
                dominio = normalizar_dominio(dominio_raw)

                if not re.match(regex, dominio):
                    mensaje = "Dominio inv√°lido."
                    return render(
                        request,
                        "analisis_dominio.html",
                        {
                            "form": form,
                            "dominios_tabla": [],
                            "mensaje": mensaje,
                            "error": None,
                            "page_obj": None,
                        },
                    )
                elif not dominio:
                    mensaje = "Dominio vac√≠o."
                    return render(
                        request,
                        "analisis_dominio.html",
                        {
                            "form": form,
                            "dominios_tabla": [],
                            "mensaje": mensaje,
                            "error": None,
                            "page_obj": None,
                        },
                    )
                else:
                    base_url = f"https://{dominio}"
                    limite_urls = request.POST.get("limite_urls")
                    try:
                        limite_urls = int(limite_urls) if limite_urls else None
                    except Exception:
                        limite_urls = None

                    resultado_crawl = crawl_urls(base_url, max_urls=limite_urls)

                    # Manejar tanto formato nuevo (dict) como antiguo (list)
                    if isinstance(resultado_crawl, dict):
                        urls_encontradas = resultado_crawl["urls"]
                        crawl_status = resultado_crawl["status"]
                        blocked_count = resultado_crawl.get('blocked_count', 0)
                    else:
                        # Compatibilidad con formato anterior
                        urls_encontradas = resultado_crawl
                        crawl_status = 'legacy'
                        blocked_count = 0

                    # Crear registro en base de datos
                    busqueda = BusquedaDominio.objects.create(
                        dominio=dominio,
                        usuario=(
                            request.user if request.user.is_authenticated else None
                        ),
                        urls="\n".join(urls_encontradas),
                    )

                    # Actualizar fecha de finalizaci√≥n
                    busqueda.fecha_fin = timezone.now()
                    busqueda.save()

                    if "dominios_buscados" not in request.session:
                        request.session["dominios_buscados"] = []

                    if dominio not in request.session["dominios_buscados"]:
                        request.session["dominios_buscados"].append(dominio)
                        request.session.modified = True

                    # Generar mensaje informativo seg√∫n el estado del crawling
                    base_msg = f"Dominio '{dominio}' analizado: {len(urls_encontradas)} URLs encontradas."

                    if crawl_status == "blocked_fallback_sitemap":
                        mensaje = f"{base_msg} ‚ö†Ô∏è Se detect√≥ protecci√≥n anti-bot, se us√≥ sitemap como alternativa."
                    elif crawl_status == "timeout_fallback_sitemap":
                        mensaje = f"{base_msg} ‚ö†Ô∏è El servidor no responde (timeouts), se us√≥ sitemap como alternativa."
                    elif crawl_status == "connection_error_fallback_sitemap":
                        mensaje = f"{base_msg} ‚ö†Ô∏è Errores de conexi√≥n, se us√≥ sitemap como alternativa."
                    elif "no_sitemap" in crawl_status:
                        mensaje = (
                            f"{base_msg} ‚ùå Crawling fall√≥ y no hay sitemap disponible."
                        )
                    elif blocked_count > 0:
                        mensaje = f"{base_msg} ‚ö†Ô∏è Se detectaron {blocked_count} bloqueos/problemas durante el crawling."
                    else:
                        mensaje = f"{base_msg} ‚úÖ Crawling completado exitosamente."

    busquedas_qs = BusquedaDominio.objects.order_by("-fecha")[:1000]
    dominios_tabla = []
    from django.utils import timezone

    for b in busquedas_qs:
        dom_norm = normalizar_dominio(b.dominio)
        fecha_inicio = timezone.localtime(b.fecha)
        fecha_fin = timezone.localtime(b.fecha_fin) if b.fecha_fin else None
        duracion = None
        estado = "En progreso"
        if fecha_fin:
            delta = fecha_fin - fecha_inicio
            total_seconds = int(delta.total_seconds())
            if total_seconds < 0:
                duracion = "-"
            else:
                h = total_seconds // 3600
                m = (total_seconds % 3600) // 60
                s = total_seconds % 60
                duracion = f"{h:02}:{m:02}:{s:02}"
            estado = "Finalizado"
        dominios_tabla.append(
            {
                "id": b.id,
                "dominio": dom_norm,
                "inicio": fecha_inicio.strftime("%Y-%m-%d %H:%M:%S"),
                "fin": fecha_fin.strftime("%Y-%m-%d %H:%M:%S") if fecha_fin else "",
                "duracion": duracion or "",
                "usuario": b.usuario.username if b.usuario else "-",
                "total_urls": len(b.get_urls()),
                "estado": estado,
                "url_original": b.dominio,
            }
        )

    paginator = Paginator(dominios_tabla, 20)
    page_number = request.GET.get("page")
    page_obj = paginator.get_page(page_number)

    total_registros = BusquedaDominio.objects.count()
    print(f"Total registros en BD: {total_registros}")
    print(f"Registros mostrados (p√°gina): {len(page_obj)}")

    for b in page_obj:
        print(
            f"  - {b['dominio']} | Inicio: {b['inicio']} | Fin: {b['fin']} | "
            f"Duraci√≥n: {b['duracion']} | URLs: {b['total_urls']}"
        )

    return render(
        request,
        "analisis_dominio.html",
        {
            "form": form,
            "dominios_tabla": list(page_obj),
            "mensaje": mensaje,
            "error": None,
            "page_obj": page_obj,
        },
    )


def analisis_detalle(request):
    """Vista para mostrar las URLs del sitemap de un dominio"""
    busqueda_id = request.GET.get("id")
    error = None
    busquedas = []
    dominio = ""
    if busqueda_id:
        try:
            busq = BusquedaDominio.objects.get(id=busqueda_id)
            busquedas = [busq]
            dominio = busq.dominio
        except BusquedaDominio.DoesNotExist:
            error = "No se encontr√≥ la b√∫squeda solicitada."
    else:
        error = "No se especific√≥ una b√∫squeda."

    form = DominioForm(initial={"dominio": dominio})
    return render(
        request,
        "analisis/detalle.html",
        {
            "form": form,
            "dominio": dominio,
            "busquedas": busquedas,
            "error": error,
        },
    )


def analisis_url_view(request):
    """Vista b√°sica para an√°lisis de una URL espec√≠fica"""
    return render(request, "analisis/url_especifica.html")


def dashboard_view(request):
    """Vista b√°sica para el dashboard"""
    return render(request, "dashboard/index.html")


def reportes_view(request):
    """Vista b√°sica para reportes"""
    return render(request, "reportes.html")


def nuevo_reporte_view(request):
    """Vista b√°sica para crear un nuevo reporte"""
    return render(request, "reportes/nuevo_reporte.html")


def nuevo_usuario_view(request):
    """Vista para crear un nuevo usuario (admin o lectura)"""
    from .forms import UsuarioLecturaForm

    mensaje = ""
    if request.method == "POST":
        form = UsuarioLecturaForm(request.POST)
        if form.is_valid():
            username = form.cleaned_data["username"]
            email = form.cleaned_data["email"]
            password = form.cleaned_data["password"]

            if User.objects.filter(username=username).exists():
                mensaje = f"El usuario '{username}' ya existe."
            elif User.objects.filter(email=email).exists():
                mensaje = f"El email '{email}' ya est√° en uso."
            else:
                is_staff = request.POST.get("is_staff") == "on"
                user = User.objects.create_user(
                    username=username, email=email, password=password
                )
                user.is_staff = is_staff
                user.save()
                mensaje = f"Usuario '{username}' creado correctamente."
                form = UsuarioLecturaForm()
        else:
            mensaje = "Corrija los errores indicados."
    else:
        form = UsuarioLecturaForm()

    usuarios = User.objects.all().order_by("-date_joined")
    return render(
        request,
        "usuarios/crear_usuario.html",
        {"form": form, "usuarios": usuarios, "mensaje": mensaje},
    )


def editar_usuarios_view(request):
    """Vista para editar usuarios con filtros, formularios y paginaci√≥n"""
    from .forms import EditarUsuarioForm

    mensaje = ""
    if request.method == "POST":
        eliminar_id = request.POST.get("eliminar_id")
        if eliminar_id:
            try:
                usuario = User.objects.get(pk=eliminar_id)
                usuario.delete()
                mensaje = "Usuario eliminado correctamente."
            except User.DoesNotExist:
                mensaje = "Usuario no encontrado para eliminar."
        else:
            user_id = request.POST.get("user_id")
            if user_id:
                try:
                    usuario = User.objects.get(pk=user_id)
                except User.DoesNotExist:
                    mensaje = "Usuario no encontrado."
                else:
                    form = EditarUsuarioForm(request.POST, instance=usuario)
                    if form.is_valid():
                        form.save()
                        mensaje = f"Usuario '{usuario.username}' actualizado."
                    else:
                        mensaje = "Error al actualizar el usuario."

    q = request.GET.get("q", "").strip()
    tipo = request.GET.get("tipo", "")
    usuarios = User.objects.all()

    if q:
        usuarios = usuarios.filter(Q(username__icontains=q) | Q(email__icontains=q))
    if tipo == "admin":
        usuarios = usuarios.filter(is_staff=True)
    elif tipo == "lectura":
        usuarios = usuarios.filter(is_staff=False)

    usuarios = usuarios.order_by("-date_joined")

    paginator = Paginator(usuarios, 20)
    page_number = request.GET.get("page")
    page_obj = paginator.get_page(page_number)

    forms_dict = {}
    for usuario in page_obj:
        forms_dict[usuario.id] = EditarUsuarioForm(instance=usuario)

    context = {
        "usuarios": page_obj,
        "forms_dict": forms_dict,
        "page_obj": page_obj,
        "mensaje": mensaje,
    }
    return render(request, "usuarios/editar_usuarios.html", context)


def soporte_view(request):
    """Vista b√°sica para soporte"""
    return render(request, "soporte.html")


def configuracion_view(request):
    """Vista b√°sica para configuraci√≥n"""
    return render(request, "configuracion.html")


def documentacion_view(request):
    """Vista b√°sica para documentaci√≥n"""
    return render(request, "documentacion.html")


def json_response_view(request):
    """Vista para respuesta JSON b√°sica"""
    return JsonResponse({"status": "ok"})


def index(request):
    """Vista principal del dashboard institucional"""
    return render(request, "dashboard/index.html")
