#!/usr/bin/env python
"""
Script de prueba para verificar la funcionalidad de recuperaci√≥n de sesi√≥n
"""

import requests
import json
from datetime import datetime


def test_estado_endpoint():
    """Probar el endpoint /analisis/estado/"""

    # Simular una petici√≥n AJAX como hace el navegador
    url = "http://localhost:8000/analisis/estado/"
    headers = {"X-Requested-With": "XMLHttpRequest", "Content-Type": "application/json"}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        print(f"Status Code: {response.status_code}")

        if response.status_code == 200:
            data = response.json()
            print("‚úÖ Respuesta JSON recibida:")
            print(json.dumps(data, indent=2, ensure_ascii=False))

            # Verificar estructura esperada
            if "progreso" in data:
                progreso = data["progreso"]
                print(f"\nüìä Datos del progreso:")
                print(f"  - Dominio: {progreso.get('dominio', 'N/A')}")
                print(f"  - Total URLs: {progreso.get('total', 0)}")
                print(f"  - Porcentaje: {progreso.get('porcentaje', 0)}%")
                print(f"  - URLs disponibles: {len(progreso.get('urls', []))}")
                print(f"  - Hora inicio: {progreso.get('hora_inicio', 'N/A')}")
                print(f"  - Hora fin: {progreso.get('hora_fin', 'N/A')}")
                print(f"  - Duraci√≥n: {progreso.get('duracion', 'N/A')}")

                if progreso.get("urls"):
                    print(f"\nüîó Primeras 3 URLs encontradas:")
                    for i, url in enumerate(progreso.get("urls", [])[:3], 1):
                        print(f"  {i}. {url}")
            else:
                print("‚ö†Ô∏è  No se encontr√≥ informaci√≥n de progreso")

        else:
            print(f"‚ùå Error HTTP: {response.status_code}")
            print(response.text[:500])

    except Exception as e:
        print(f"‚ùå Error en la petici√≥n: {str(e)}")


def test_crawling_activo():
    """Probar el endpoint /crawling/activo/"""

    url = "http://localhost:8000/crawling/activo/"

    try:
        response = requests.get(url, timeout=10)
        print(f"\nüîÑ Endpoint crawling activo - Status: {response.status_code}")

        if response.status_code == 200:
            data = response.json()
            print("Respuesta:")
            print(json.dumps(data, indent=2, ensure_ascii=False))
        else:
            print(f"Error: {response.text[:200]}")

    except Exception as e:
        print(f"Error: {str(e)}")


def main():
    print("üß™ PRUEBA DE RECUPERACI√ìN DE SESI√ìN")
    print("=" * 50)
    print(f"Hora de prueba: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    print("1Ô∏è‚É£  Probando endpoint /analisis/estado/")
    print("-" * 30)
    test_estado_endpoint()

    print("\n2Ô∏è‚É£  Probando endpoint /crawling/activo/")
    print("-" * 30)
    test_crawling_activo()

    print("\n‚úÖ Pruebas completadas")
    print("\nINSTRUCCIONES PARA PRUEBA MANUAL:")
    print("1. Abre el navegador en http://localhost:8000")
    print("2. Inicia un crawling de argentina.gob.ar")
    print("3. Espera a que termine (ver√°s 'Finalizado correctamente')")
    print("4. Cierra la pesta√±a del navegador")
    print("5. Abre una nueva pesta√±a en http://localhost:8000")
    print("6. Deber√≠as ver todo el progreso completo visible:")
    print("   - Barra de progreso al 100%")
    print("   - Informaci√≥n de tiempo (inicio, fin, duraci√≥n)")
    print("   - Contador de URLs encontradas")
    print("   - Lista completa de URLs")
    print("   - Estado 'Completado' con ‚úì verde")


if __name__ == "__main__":
    main()
